{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Llama 3 8B using JAX\n",
        "\n",
        "Converted from this [PyTorch Lightning tutorial](https://lightning.ai/fareedhassankhan12/studios/building-llama-3-from-scratch) to use JAX.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import tiktoken\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "import torch\n",
        "import json\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "\n",
        "path_to_model = \"path_to_llama-3-8B_original_pytorch_weights\"\n",
        "tokenizer_model = load_tiktoken_bpe(path_to_model+\"tokenizer.model\")\n",
        "model_weights = torch.load(path_to_model+\"consolidated.00.pth\")\n",
        "\n",
        "\n",
        "with open(path_to_model+\"params.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "dim = config[\"dim\"]\n",
        "n_layers = config[\"n_layers\"]\n",
        "n_heads = config[\"n_heads\"]\n",
        "n_kv_heads = config[\"n_kv_heads\"]\n",
        "vocab_size = config[\"vocab_size\"]\n",
        "multiple_of = config[\"multiple_of\"]\n",
        "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
        "norm_eps = config[\"norm_eps\"]\n",
        "rope_theta = config[\"rope_theta\"]\n",
        "\n",
        "special_tokens = [\n",
        "    \"<|begin_of_text|>\",\n",
        "    \"<|end_of_text|>\",\n",
        "    \"<|reserved_special_token_0|>\",\n",
        "    \"<|reserved_special_token_1|>\",\n",
        "    \"<|reserved_special_token_2|>\",\n",
        "    \"<|reserved_special_token_3|>\",\n",
        "    \"<|start_header_id|>\",\n",
        "    \"<|end_header_id|>\",\n",
        "    \"<|reserved_special_token_4|>\",\n",
        "    \"<|eot_id|>\",\n",
        "] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n",
        "\n",
        "tokenize_breaker = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n",
        "\n",
        "tokenizer = tiktoken.Encoding(\n",
        "    name = path_to_model+\"tokenizer.model\",\n",
        "    pat_str = tokenize_breaker,\n",
        "    mergeable_ranks = tokenizer_model,\n",
        "    special_tokens={token: len(tokenizer_model) + i for i, token in enumerate(special_tokens)},\n",
        ")\n",
        "\n",
        "# prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
        "prompt = \"the capital of China is \"\n",
        "tokens = [128000] + tokenizer.encode(prompt)\n",
        "\n",
        "\n",
        "embedding_layer = nnx.Embed(vocab_size, dim, rngs=nnx.Rngs(0))\n",
        "embedding_layer.embedding.value = model_weights[\"tok_embeddings.weight\"].float().numpy()\n",
        "token_embeddings_unnormalized = embedding_layer(jnp.asarray(tokens)).astype(jnp.bfloat16)\n",
        "\n",
        "\n",
        "def rms_norm(tensor, norm_weights):\n",
        "    squared_mean = jnp.mean(jnp.square(tensor), axis=-1, keepdims=True)\n",
        "    normalized = jnp.reciprocal(jnp.sqrt(squared_mean + norm_eps))\n",
        "    return (tensor * normalized) * norm_weights\n",
        "\n",
        "head_dim = dim // n_heads\n",
        "\n",
        "#TODO: 64 here needs to come from somewhere\n",
        "zero_to_one_split_into_64_parts = jnp.arange(64)/64\n",
        "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
        "freqs_for_each_token = jnp.outer(jnp.arange(token_embeddings_unnormalized.shape[0]), freqs)\n",
        "freqs_cis = jnp.complex64(jnp.exp(1j * freqs))\n",
        "\n",
        "hidden_state = token_embeddings_unnormalized\n",
        "\n",
        "for layer in range(n_layers):\n",
        "    qkv_attention_store = []\n",
        "\n",
        "    layer_embedding_norm = rms_norm(hidden_state, jnp.asarray(model_weights[f\"layers.{layer}.attention_norm.weight\"].float().numpy()).astype(jnp.bfloat16))\n",
        "\n",
        "    q_layer = jnp.asarray(model_weights[f\"layers.{layer}.attention.wq.weight\"].float().numpy()).astype(jnp.bfloat16)\n",
        "    q_layer = jnp.reshape(q_layer, (n_heads, q_layer.shape[0] // n_heads, dim))\n",
        "    k_layer = jnp.asarray(model_weights[f\"layers.{layer}.attention.wk.weight\"].float().numpy()).astype(jnp.bfloat16)\n",
        "    k_layer = k_layer.reshape(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
        "    v_layer = jnp.asarray(model_weights[f\"layers.{layer}.attention.wv.weight\"].float().numpy()).astype(jnp.bfloat16)\n",
        "    v_layer = v_layer.reshape(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
        "    w_layer = jnp.asarray(model_weights[f\"layers.{layer}.attention.wo.weight\"].float().numpy()).astype(jnp.bfloat16)\n",
        "\n",
        "    for head in range(n_heads):\n",
        "        q_layer_head = q_layer[head]\n",
        "        k_layer_head = k_layer[head//4]\n",
        "        v_layer_head = v_layer[head//4]\n",
        "\n",
        "        q_per_token = jnp.matmul(layer_embedding_norm, q_layer_head.T)\n",
        "        k_per_token = jnp.matmul(layer_embedding_norm, k_layer_head.T)\n",
        "        v_per_token = jnp.matmul(layer_embedding_norm, v_layer_head.T)\n",
        "\n",
        "        # apply RoPe below\n",
        "        freqs_for_each_token = jnp.outer(jnp.arange(token_embeddings_unnormalized.shape[0]), freqs)\n",
        "        freqs_cis = jnp.exp(1j * freqs_for_each_token)\n",
        "        q_per_token_split_into_pairs = q_per_token.astype(jnp.float32).reshape(q_per_token.shape[0], -1, 2)\n",
        "        q_per_token_as_complex_numbers = q_per_token_split_into_pairs[..., 0] + 1j * q_per_token_split_into_pairs[..., 1]\n",
        "        q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis\n",
        "        q_per_token_split_into_pairs_rotated = jnp.stack([q_per_token_as_complex_numbers_rotated.real, q_per_token_as_complex_numbers_rotated.imag], axis=-1)\n",
        "        q_per_token_rotated = q_per_token_split_into_pairs_rotated.reshape(q_per_token.shape)\n",
        "\n",
        "        # Repeat the process for k_per_token\n",
        "        k_per_token_split_into_pairs = k_per_token.astype(jnp.float32).reshape(k_per_token.shape[0], -1, 2)\n",
        "        k_per_token_as_complex_numbers = k_per_token_split_into_pairs[..., 0] + 1j * k_per_token_split_into_pairs[..., 1]\n",
        "        k_per_token_as_complex_numbers_rotated = k_per_token_as_complex_numbers * freqs_cis\n",
        "        k_per_token_split_into_pairs_rotated = jnp.stack([k_per_token_as_complex_numbers_rotated.real, k_per_token_as_complex_numbers_rotated.imag], axis=-1)\n",
        "        k_per_token_rotated = k_per_token_split_into_pairs_rotated.reshape(k_per_token.shape)\n",
        "\n",
        "        # TODO: update 128\n",
        "        qk_per_token = jnp.matmul(q_per_token_rotated, k_per_token_rotated.T) / (128) ** 0.5\n",
        "\n",
        "        mask = jnp.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(\"-inf\"))\n",
        "        mask = jnp.triu(mask, k=1)\n",
        "        qk_per_token_after_masking = qk_per_token + mask\n",
        "\n",
        "        qk_per_token_after_masking_after_softmax = jax.nn.softmax(qk_per_token_after_masking, axis=1)\n",
        "\n",
        "        qkv_attention = jnp.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
        "\n",
        "        qkv_attention_store.append(qkv_attention)\n",
        "\n",
        "    stacked_qkv_attention = jnp.concatenate(qkv_attention_store, axis=-1)\n",
        "\n",
        "    embedding_delta = jnp.matmul(stacked_qkv_attention, w_layer.T)\n",
        "\n",
        "    embedding_after_edit = hidden_state + embedding_delta\n",
        "\n",
        "    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model_weights[f\"layers.{layer}.ffn_norm.weight\"].float().numpy())\n",
        "\n",
        "    w1 = jnp.asarray(model_weights[f\"layers.{layer}.feed_forward.w1.weight\"].float().numpy()).astype(jnp.bfloat16)\n",
        "    w2 = jnp.asarray(model_weights[f\"layers.{layer}.feed_forward.w2.weight\"].float().numpy()).astype(jnp.bfloat16)\n",
        "    w3 = jnp.asarray(model_weights[f\"layers.{layer}.feed_forward.w3.weight\"].float().numpy()).astype(jnp.bfloat16)\n",
        "\n",
        "    output_after_feedforward = jnp.matmul(nnx.silu(jnp.matmul(embedding_after_edit_normalized, w1.T)) * jnp.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
        "\n",
        "    hidden_state = embedding_after_edit + output_after_feedforward\n",
        "\n",
        "logits = jnp.matmul(hidden_state[-1], jnp.asarray(model_weights[\"output.weight\"].float().numpy()).astype(jnp.bfloat16).T)\n",
        "\n",
        "next_token = jnp.argmax(logits, axis=-1)\n",
        "\n",
        "print(tokenizer.decode([next_token]))"
      ],
      "metadata": {
        "id": "jJ8wCbgevWFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}