{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a direct translation of [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) from Keras to JAX/Flax."
      ],
      "metadata": {
        "id": "rvP1eNN_pExM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install JAX and Flax"
      ],
      "metadata": {
        "id": "hTmz5Cbco7n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"jax[cuda12]\" flax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "6f950ee0-2a85-4b35-e829-d8c708cd8c86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.8.4)\n",
            "Collecting flax\n",
            "  Downloading flax-0.8.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: jax[cuda12] in /usr/local/lib/python3.10/dist-packages (0.4.26)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax[cuda12])\n",
            "  Downloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (1.13.1)\n",
            "Collecting jax-cuda12-plugin<=0.4.31,>=0.4.31 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_plugin-0.4.31-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.5.23)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.64)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.2)\n",
            "Collecting jax-cuda12-pjrt==0.4.31 (from jax-cuda12-plugin<=0.4.31,>=0.4.31->jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_pjrt-0.4.31-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
            "Collecting nvidia-cublas-cu12>=12.1.3.1 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cublas_cu12-12.6.0.22-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.37-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.37-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu12<10.0,>=9.1 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cudnn_cu12-9.3.0.75-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu12>=11.0.2.54 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cufft_cu12-11.2.6.28-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12>=11.4.5.107 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.4.38-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12>=12.1.0.106 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cusparse_cu12-12.5.2.23-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12>=2.18.1 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.31,>=0.4.31; extra == \"cuda12\"->jax[cuda12])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (3.20.3)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.19.2)\n",
            "Downloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl (88.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flax-0.8.5-py3-none-any.whl (731 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.3/731.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.31-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.4.31-cp310-cp310-manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.4.31-py3-none-manylinux2014_x86_64.whl (84.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.0.22-py3-none-manylinux2014_x86_64.whl (368.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.0/368.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.37-py3-none-manylinux2014_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.37-py3-none-manylinux2014_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.3.0.75-py3-none-manylinux2014_x86_64.whl (577.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.2/577.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.6.28-py3-none-manylinux2014_x86_64.whl (192.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.4.38-py3-none-manylinux2014_x86_64.whl (130.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.2.23-py3-none-manylinux2014_x86_64.whl (217.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.5/217.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: jax-cuda12-pjrt, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jax-cuda12-plugin, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jaxlib, nvidia-cusolver-cu12, jax, flax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.4\n",
            "    Uninstalling flax-0.8.4:\n",
            "      Successfully uninstalled flax-0.8.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.6.0.22 which is incompatible.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.6.37 which is incompatible.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.6.37 which is incompatible.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.6.28 which is incompatible.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.4.38 which is incompatible.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.2.23 which is incompatible.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flax-0.8.5 jax-0.4.31 jax-cuda12-pjrt-0.4.31 jax-cuda12-plugin-0.4.31 jaxlib-0.4.31 nvidia-cublas-cu12-12.6.0.22 nvidia-cuda-cupti-cu12-12.6.37 nvidia-cuda-nvcc-cu12-12.6.20 nvidia-cuda-runtime-cu12-12.6.37 nvidia-cudnn-cu12-9.3.0.75 nvidia-cufft-cu12-11.2.6.28 nvidia-cusolver-cu12-11.6.4.38 nvidia-cusparse-cu12-12.5.2.23 nvidia-nccl-cu12-2.22.3 nvidia-nvjitlink-cu12-12.6.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the training data."
      ],
      "metadata": {
        "id": "OHzJ_bokoovZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olwq3MrpojcJ",
        "outputId": "360ce4d7-e39b-4965-9e99-4903304adb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  10.4M      0  0:00:07  0:00:07 --:--:-- 16.2M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and train the model."
      ],
      "metadata": {
        "id": "rPyt7MV6prz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from typing import Any, Callable\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from flax.training import train_state\n",
        "import keras\n",
        "\n",
        "\n",
        "def causal_attention_mask(seq_len):\n",
        "    \"\"\"\n",
        "    Generates a causal attention mask for self-attention.\n",
        "    \"\"\"\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    embed_dim: int\n",
        "    num_heads: int\n",
        "    ff_dim: int\n",
        "    rate: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        batch_size, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = nn.MultiHeadAttention(num_heads=self.num_heads)(\n",
        "            inputs_q=inputs,\n",
        "            inputs_kv=inputs,\n",
        "            mask=mask\n",
        "        )\n",
        "        attention_output = nn.Dropout(rate=self.rate)(attention_output, deterministic=not training)\n",
        "        out1 = nn.LayerNorm(epsilon=1e-6)(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = nn.Dense(features=self.ff_dim)(out1)\n",
        "        ffn_output = nn.relu(ffn_output)\n",
        "        ffn_output = nn.Dense(features=self.embed_dim)(ffn_output)\n",
        "        ffn_output = nn.Dropout(rate=self.rate)(ffn_output, deterministic=not training)\n",
        "\n",
        "        return nn.LayerNorm(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    maxlen: int\n",
        "    vocab_size: int\n",
        "    embed_dim: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, self.maxlen)[None, :]\n",
        "        position_embedding = nn.Embed(self.maxlen, self.embed_dim)(positions)\n",
        "        token_embedding = nn.Embed(int(self.vocab_size), self.embed_dim)(x)\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    maxlen: int\n",
        "    vocab_size: int\n",
        "    embed_dim: int\n",
        "    num_heads: int\n",
        "    feed_forward_dim: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        embedding_layer = TokenAndPositionEmbedding(\n",
        "            self.maxlen, self.vocab_size, self.embed_dim\n",
        "        )\n",
        "        x = embedding_layer(inputs)\n",
        "        transformer_block = TransformerBlock(\n",
        "            self.embed_dim, self.num_heads, self.feed_forward_dim\n",
        "        )\n",
        "        x = transformer_block(x, training=training)\n",
        "        outputs = nn.Dense(features=self.vocab_size)(x)\n",
        "        return outputs, x\n",
        "\n",
        "\n",
        "vocab_size = 20000\n",
        "maxlen = 80\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "feed_forward_dim = 256\n",
        "batch_size = 640\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim)\n",
        "\n",
        "\n",
        "# Data loading and preprocessing\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"./aclImdb/train/pos\",\n",
        "    \"./aclImdb/train/neg\",\n",
        "    \"./aclImdb/test/pos\",\n",
        "    \"./aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
        "\n",
        "\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# JAX doesn't have a direct equivalent to Keras callbacks, so we'll implement the text generation as a separate function\n",
        "def generate_text(params, max_tokens, start_tokens, index_to_word, top_k=10):\n",
        "    model = create_model()\n",
        "\n",
        "    def sample_from(logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = jax.nn.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    def generate_step(start_tokens):\n",
        "        pad_len = maxlen - len(start_tokens)\n",
        "        sample_index = len(start_tokens) - 1\n",
        "        if pad_len < 0:\n",
        "            x = jnp.array(start_tokens[:maxlen])\n",
        "            sample_index = maxlen - 1\n",
        "        elif pad_len > 0:\n",
        "            x = jnp.array(start_tokens + [0] * pad_len)\n",
        "        else:\n",
        "            x = jnp.array(start_tokens)\n",
        "\n",
        "        x = x[None, :]\n",
        "        logits, _ = model.apply({\"params\": params}, x)\n",
        "        next_token = sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    generated = []\n",
        "    for _ in range(max_tokens):\n",
        "        next_token = generate_step(start_tokens + generated)\n",
        "        generated.append(int(next_token))\n",
        "    return \" \".join([index_to_word[token] for token in start_tokens + generated])\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def create_train_state(rng):\n",
        "    model = create_model()\n",
        "    params = model.init(rng, jnp.ones((1, maxlen), dtype=jnp.int32))[\"params\"]\n",
        "    tx = optax.adam(learning_rate=1e-3)\n",
        "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, batch):\n",
        "    def loss_fn(params):\n",
        "        logits, _ = state.apply_fn({\"params\": params}, batch[0])\n",
        "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[1]).mean()\n",
        "        return loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "state = create_train_state(rng)\n",
        "\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in text_ds:\n",
        "        batch = (jnp.array(batch[0].numpy()), jnp.array(batch[1].numpy()))\n",
        "        state, loss = train_step(state, batch)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
        "    start_prompt = \"this movie is\"\n",
        "    start_tokens = [\n",
        "        vectorize_layer.get_vocabulary().index(word)\n",
        "        for word in start_prompt.split()\n",
        "    ]\n",
        "    generated_text = generate_text(\n",
        "        state.params, 40, start_tokens, vectorize_layer.get_vocabulary()\n",
        "    )\n",
        "    print(f\"Generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "# Final text generation\n",
        "start_tokens = [\n",
        "    vectorize_layer.get_vocabulary().index(word) for word in start_prompt.split()\n",
        "]\n",
        "generated_text = generate_text(\n",
        "    state.params, 40, start_tokens, vectorize_layer.get_vocabulary()\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExIFGGGMpk3O",
        "outputId": "5f669b26-421c-45d8-f982-b5d7a206cc10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 files\n",
            "Epoch 1, Loss: 5.580131530761719\n",
            "Generated text:\n",
            "this movie is one of the first , the story , the story , but it is the first , and i 'm a great movie is not . . . . it is not . . . i think . the story\n",
            "\n",
            "Epoch 2, Loss: 5.24554443359375\n",
            "Generated text:\n",
            "this movie is the worst i saw this movie was not seen it 's . it . it is the acting . the acting . the acting was not funny and it is not to see a very well as it was very\n",
            "\n",
            "Epoch 3, Loss: 4.789714336395264\n",
            "Generated text:\n",
            "this movie is one of the best ever seen in a good . i 'm a few movies that this film . it 's a good film . the plot was very well as i can 't have to say it . it\n",
            "\n",
            "Epoch 4, Loss: 4.74907112121582\n",
            "Generated text:\n",
            "this movie is not one of the worst films ever made , and it was so funny , i had seen it and was very well done , and the story line was so funny , i thought the plot . it was\n",
            "\n",
            "Epoch 5, Loss: 4.466032028198242\n",
            "Generated text:\n",
            "this movie is one of the worst films ever made , and it was so awful that it was so awful , it was made by far better . the story is the story line that was made by a lot of people\n",
            "\n",
            "Epoch 6, Loss: 4.374608039855957\n",
            "Generated text:\n",
            "this movie is not one of the worst films ever made . the film has no plot . i don 't understand the movie that is so much of it . it 's not just as bad as it is the most part\n",
            "\n",
            "Epoch 7, Loss: 4.310944080352783\n",
            "Generated text:\n",
            "this movie is not one of the worst films i have ever seen . it 's not just bad , but the only reason why it is it ? it is that i don 't think of it is . the plot is\n",
            "\n",
            "Epoch 8, Loss: 4.096940994262695\n",
            "Generated text:\n",
            "this movie is so awful it 's a very funny film . the only reason it was the first one . it 's not funny but the acting was bad , and i 'm a huge fan of the first movie . the\n",
            "\n",
            "Epoch 9, Loss: 3.8878979682922363\n",
            "Generated text:\n",
            "this movie is not one of the worst films i have ever seen . it 's not funny . the plot is ridiculous . i can 't think it is not only in any way it 's just the worst film i have\n",
            "\n",
            "Epoch 10, Loss: 3.8851215839385986\n",
            "Generated text:\n",
            "this movie is not only in my opinion . i 'm a big fan of the [UNK] and the plot , i think that was so much as the main characters are [UNK] and the plot of this movie was not just as\n",
            "\n",
            "Epoch 11, Loss: 3.815979480743408\n",
            "Generated text:\n",
            "this movie is so funny and i don 't think i have to say it 's funny , i am very disappointed that this movie has not been a big fan . the story was so good . it 's just about a\n",
            "\n",
            "Epoch 12, Loss: 3.7041866779327393\n",
            "Generated text:\n",
            "this movie is the best of all time . the actors are good , the characters and the movie is good , the plot and the characters are so wooden , they are very well [UNK] . i am a fan of the\n",
            "\n",
            "Epoch 13, Loss: 3.6104578971862793\n",
            "Generated text:\n",
            "this movie is not just plain awful . it 's a good story and it is not funny . the only way the acting is terrible . it 's the worst acting , it 's all that i can 't understand . it\n",
            "\n",
            "Epoch 14, Loss: 3.4936330318450928\n",
            "Generated text:\n",
            "this movie is about the worst acting ever made , but the story is awful . it 's just plain stupid , stupid and unfunny . it is so bad . it is so bad it 's a shame . i don 't\n",
            "\n",
            "Epoch 15, Loss: 3.4107584953308105\n",
            "Generated text:\n",
            "this movie is one of the worst movies i have ever seen . it 's just plain and i think i would like to watch this movie because i think i have to say it 's very bad , and i can 't\n",
            "\n",
            "Epoch 16, Loss: 3.468989133834839\n",
            "Generated text:\n",
            "this movie is about the [UNK] who [UNK] and [UNK] it all has [UNK] the world \" and it is very similar to which i 've been able to sit through this movie and get the chance to get to see it again\n",
            "\n",
            "Epoch 17, Loss: 3.3448381423950195\n",
            "Generated text:\n",
            "this movie is about the same family who is not in the past but the same year the [UNK] as well as the father , it 's also very best , i 've ever seen , the first two of the movies ,\n",
            "\n",
            "Epoch 18, Loss: 3.2000768184661865\n",
            "Generated text:\n",
            "this movie is one of the worst movies i have ever seen . the plot is very predictable and the characters . i don 't understand how much of it , especially when the guy got [UNK] , it was the first of\n",
            "\n",
            "Epoch 19, Loss: 3.2067790031433105\n",
            "Generated text:\n",
            "this movie is not as bad as the book , it 's just that the whole thing was the right hand out . the movie was the best way to see . i am not sure the movie is , it 's not\n",
            "\n",
            "Epoch 20, Loss: 3.120828151702881\n",
            "Generated text:\n",
            "this movie is about the [UNK] . the movie is very bad , but the story is just about it . the movie itself is not for you . the main character is not to mention the character . it is not to\n",
            "\n",
            "Epoch 21, Loss: 2.9950807094573975\n",
            "Generated text:\n",
            "this movie is about the best i 've seen since . i am not sure it 's not funny . i 'm a huge fan of the genre and i love the horror series , but this was just the same type i\n",
            "\n",
            "Epoch 22, Loss: 3.0593199729919434\n",
            "Generated text:\n",
            "this movie is about the best of the best things i 've ever seen . i can 't say it 's the first movie , i was not sure how it would be possible to get . the movie was like it .\n",
            "\n",
            "Epoch 23, Loss: 2.9615418910980225\n",
            "Generated text:\n",
            "this movie is about the [UNK] and the best of all time . the cast is perfect for this movie , but the plot is just as a joke . it is very hard to find the acting [UNK] [UNK] the story is\n",
            "\n",
            "Epoch 24, Loss: 2.881340980529785\n",
            "Generated text:\n",
            "this movie is not the best i have seen . i love it all of the way . i was a huge fan of the movie i 've ever seen , so i could watch it . it 's hard to say it\n",
            "\n",
            "Epoch 25, Loss: 2.8538296222686768\n",
            "Generated text:\n",
            "this movie is very very funny and heartwarming about the life of the movie . i wish i had a movie more . the story was good . the movie i had no idea how this movie was , i thought that the\n",
            "\n",
            "Final generated text:\n",
            "this movie is very very funny and heartwarming about the life of the movie . i wish i had a movie more . the story was good . the movie i had no idea how this movie was , i thought that the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
