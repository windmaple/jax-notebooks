{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP1eNN_pExM"
      },
      "source": [
        "This is a direct translation of the [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) tutorial from Keras to JAX. It aims to teach developers who are familiar with Keras/Tensorflow to pick up JAX/Flax quickly.\n",
        "\n",
        "This notebook demonstrates how to use [Flax NNX](https://flax.readthedocs.io/en/latest/nnx/index.html) to implement an autoregressive language model using a miniaturized version of the GPT model. The model uses only a single transformer block and is easy to understand.\n",
        "\n",
        "It is assumed that Colab T4 is used to run this notebook. Adjust the batch size if another hardware is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "8d26acb0-5f86-4b57-c9e6-e46f372c52df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jax-ai-stack\n",
            "  Downloading jax_ai_stack-2024.10.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: jax==0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax-ai-stack) (0.4.33)\n",
            "Collecting flax==0.9.0 (from jax-ai-stack)\n",
            "  Downloading flax-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ml-dtypes==0.4.0 (from jax-ai-stack)\n",
            "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: optax==0.2.3 in /usr/local/lib/python3.10/dist-packages (from jax-ai-stack) (0.2.3)\n",
            "Requirement already satisfied: orbax-checkpoint==0.6.4 in /usr/local/lib/python3.10/dist-packages (from jax-ai-stack) (0.6.4)\n",
            "Collecting orbax-export==0.0.5 (from jax-ai-stack)\n",
            "  Downloading orbax_export-0.0.5-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (1.0.8)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (0.1.66)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (13.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (6.0.2)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.33->jax-ai-stack) (0.4.33)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.33->jax-ai-stack) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.33->jax-ai-stack) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.33->jax-ai-stack) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (0.1.87)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (1.9.4)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.6.4->jax-ai-stack) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.6.4->jax-ai-stack) (3.20.3)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.6.4->jax-ai-stack) (4.10.0)\n",
            "Collecting dataclasses-json (from orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jaxtyping (from orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading jaxtyping-0.2.34-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax==0.2.3->jax-ai-stack) (0.12.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.9.0->jax-ai-stack) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.9.0->jax-ai-stack) (2.18.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.6.4->jax-ai-stack) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.6.4->jax-ai-stack) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.6.4->jax-ai-stack) (3.20.2)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.9.0->jax-ai-stack) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->orbax-export==0.0.5->jax-ai-stack) (24.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading jax_ai_stack-2024.10.1-py3-none-any.whl (10 kB)\n",
            "Downloading flax-0.9.0-py3-none-any.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.7/780.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_export-0.0.5-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jaxtyping-0.2.34-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typeguard, mypy-extensions, ml-dtypes, marshmallow, typing-inspect, jaxtyping, dataclasses-json, orbax-export, flax, jax-ai-stack\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.5\n",
            "    Uninstalling flax-0.8.5:\n",
            "      Successfully uninstalled flax-0.8.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 flax-0.9.0 jax-ai-stack-2024.10.1 jaxtyping-0.2.34 marshmallow-3.22.0 ml-dtypes-0.4.0 mypy-extensions-1.0.0 orbax-export-0.0.5 typeguard-2.13.3 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jax-ai-stack\n",
        "!pip install -U \"jax[cuda12]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzJ_bokoovZ"
      },
      "source": [
        "Grab the IMDB review data as the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olwq3MrpojcJ",
        "outputId": "f07925b6-128b-48e8-d332-0987b9670db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  1447k      0  0:00:56  0:00:56 --:--:-- 1242k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "from typing import Any\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Next, defne the model architecture, which is a decoder-only transformer model. The model is similar to the GPT model series but it's smaller in size with only one transformer block, which is why we are calling it miniGPT. The model has several key components stacked up together, so let's go over the them one by one.\n",
        "\n",
        "The key component is the `TransformerBlock`, which uses the multi-head attention mechanism as described in the famous [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper. Please get familiar with the paper if you are not already because we are going to implement some of the details below.\n",
        "\n",
        "The model is auto-regressive, so it can only attend to previous tokens. So we use [`jax.numpy.tril`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tril.html) to create the attention mask, and pass it in the `nnx.MultiHeadAttention` layer. The other layers follow the practice of the decoder layer in the paper.\n",
        "\n",
        "All layers (except `Dropout`) has a `rngs` parameter, which is the [random generator key](https://jax.readthedocs.io/en/latest/jax.random.html#prng-keys) that can help you reproduce results and debug issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads, in_features=embed_dim, rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6, num_features=embed_dim, rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim, out_features=ff_dim, rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim, out_features=embed_dim, rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6, num_features=embed_dim, rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        batch_size, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.linear1(out1)\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "\n",
        "        return self.layer_norm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVA647SA8mQT"
      },
      "source": [
        "Since the model input is just text tokens, we need to convert them into embeddings. We use two kinds of embeddings: token embedding and position embeddings, both of which are learned by the model and are added up. Note that this is slightly different from the origianl paper, which uses static, instead of learned, positional embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywxWh4cg5Kh2"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return token_embedding + position_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUTg9IxJ8-Q1"
      },
      "source": [
        "Now we can put everything together to build our miniGPT model. We convert the tokens into embeddings, add a single `TransformerBlock` and finally use a linear projection layer for output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmUaAvr75SvU"
      },
      "outputs": [],
      "source": [
        "class MiniGPT(nnx.Module):\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        )\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim, out_features=vocab_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "maxlen = 80\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "feed_forward_dim = 256\n",
        "batch_size = 512 # for Colab T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Data loading and preprocessing. To map the words and symbols to indices, we need to tokenize them first. For simplicity, we are using a vey simple tokenization scheme:\n",
        "* The `custom_standardization` function does some preprocessing by removing undesirable symbols and adding space before punctuations, so that punctuations can be treated as tokens like words\n",
        "* The `build_vocab` function builds our own vocaulary according to the `vocab_size` defined above\n",
        "* The `tokenize` function does the tokenization\n",
        "* We also batch the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUFsn1GMuzh",
        "outputId": "0a7e757a-0fc0-49f0-909e-7086163c62dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "# Data loading and preprocessing\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"./aclImdb/train/pos\",\n",
        "    \"./aclImdb/train/neg\",\n",
        "    \"./aclImdb/test/pos\",\n",
        "    \"./aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Custom text processing: add space before and after punctuations for tokenization\n",
        "def custom_standardization(input_string):\n",
        "    lowercased = input_string.lower()\n",
        "    stripped_html = lowercased.replace(\"<br />\", \" \")\n",
        "    return ''.join([' ' + char + ' ' if char in string.punctuation else char for char in stripped_html]).strip()\n",
        "\n",
        "def build_vocab(texts, vocab_size):\n",
        "    all_words = ' '.join(texts).split()\n",
        "    word_counts = Counter(all_words)\n",
        "    vocab = ['<PAD>', '<UNK>'] + [word for word, _ in word_counts.most_common(vocab_size - 2)]\n",
        "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "    return vocab, word_to_index\n",
        "\n",
        "def tokenize(text, word_to_index, maxlen):\n",
        "    words = text.split()\n",
        "    tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in words]\n",
        "    if len(tokens) < maxlen:\n",
        "        tokens = tokens + [word_to_index['<PAD>']] * (maxlen - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:maxlen]\n",
        "    return tokens\n",
        "\n",
        "def load_and_preprocess_data(filenames, batch_size, vocab_size, maxlen):\n",
        "    data = []\n",
        "    for filename in filenames:\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            processed_text = custom_standardization(text)\n",
        "            data.append(processed_text)\n",
        "\n",
        "    vocab, word_to_index = build_vocab(data, vocab_size)\n",
        "    tokenized_data = [tokenize(text, word_to_index, maxlen) for text in data]\n",
        "\n",
        "    # Batch the data\n",
        "    batched_data = [tokenized_data[i:i+batch_size] for i in range(0, len(tokenized_data), batch_size)]\n",
        "\n",
        "    return batched_data, vocab, word_to_index\n",
        "\n",
        "text_ds, vocab, word_to_index = load_and_preprocess_data(filenames, batch_size, vocab_size, maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Define a helper function for generating text given a model and prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f4rEMm4M5lg"
      },
      "outputs": [],
      "source": [
        "def generate_text(model: MiniGPT, max_tokens: int, start_tokens: [int], index_to_word: [str], top_k=10):\n",
        "    def sample_from(logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    def generate_step(start_tokens):\n",
        "        pad_len = maxlen - len(start_tokens)\n",
        "        sample_index = len(start_tokens) - 1\n",
        "        if pad_len < 0:\n",
        "            x = jnp.array(start_tokens[:maxlen])\n",
        "            sample_index = maxlen - 1\n",
        "        elif pad_len > 0:\n",
        "            x = jnp.array(start_tokens + [0] * pad_len)\n",
        "        else:\n",
        "            x = jnp.array(start_tokens)\n",
        "\n",
        "        x = x[None, :]\n",
        "        logits = model(x)\n",
        "        next_token = sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    generated = []\n",
        "    for _ in range(max_tokens):\n",
        "        next_token = generate_step(start_tokens + generated)\n",
        "        generated.append(int(next_token))\n",
        "    return \" \".join([index_to_word[token] for token in start_tokens + generated])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkuaFXkANFNp"
      },
      "source": [
        "Define the loss function and training step function. The `train_step` is usually the most expensive function since it needs to compute the gradients and update the model parameters. We can use [JAX JIT compilation](https://jax.readthedocs.io/en/latest/jit-compilation.html#jit-compiling-a-function) to accelerate the execution of this function, but since we using NNX here, we annoate it with `@nnx.jit` instead of `@jax.jit`. JIT-compiled functions sometimes are tricky to debug; please refer to our [debugging documentation](https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html#compiled-prints-and-breakpoints) for help if you encouter such a situation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "Start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysl6CsfENeJN",
        "outputId": "dff174d5-1676-4643-f59b-5b73adee5552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial generated text:\n",
            "this movie is seem shameless celebrity clarity claudio xmas reunion drafted weed capability distrust perlman impaled nominal hesitate inside colleges wage supervision kerry thrillers celeste activists supporter partisan filled rookie sneak bona erase urban rowlands damp daria islanders english overshadowed overheard jazz cheezy\n",
            "\n",
            "Epoch 1, Loss: 6.119418144226074\n",
            "Generated text:\n",
            "this movie is not that the story , the best , and it was not sure i think the first , and it ' t really , it is one of the first time , i have seen the story . it is\n",
            "\n",
            "Epoch 2, Loss: 5.050351619720459\n",
            "Generated text:\n",
            "this movie is the movie that it ' s best , and the acting was very well - and i thought that the plot . the acting was just about a great acting is very bad movie was so much about a great\n",
            "\n",
            "Epoch 3, Loss: 4.757399082183838\n",
            "Generated text:\n",
            "this movie is not only to the movie , it is the best , i have seen in a film , i was not to watch this film , and the only reason to watch this film , i saw this film on\n",
            "\n",
            "Epoch 4, Loss: 4.550055027008057\n",
            "Generated text:\n",
            "this movie is the worst of the best film i ' ve ever seen . it is the best , i ' ve ever seen in my life and i was really excited to watch this film , but it was just plain\n",
            "\n",
            "Epoch 5, Loss: 4.4076080322265625\n",
            "Generated text:\n",
            "this movie is the worst of the best ever . it is not the best film . the story is very bad , the story of the acting was great , but it ' s just the same movie , but i don\n",
            "\n",
            "Epoch 6, Loss: 4.296298980712891\n",
            "Generated text:\n",
            "this movie is so bad , i have no idea that i would love this film . it is the first one of the worst films ever made . it is the only thing i can say i have seen the film .\n",
            "\n",
            "Epoch 7, Loss: 4.203237533569336\n",
            "Generated text:\n",
            "this movie is one of the best of all time , but the film was in my life and i thought i would like this . it ' s not funny . the only reason why the film did i would be <UNK>\n",
            "\n",
            "Epoch 8, Loss: 4.122411727905273\n",
            "Generated text:\n",
            "this movie is one of the best i ' ve ever seen . the acting was horrible , the movie was just bad . i thought that this was going to be the best film . the story is very weak . i\n",
            "\n",
            "Epoch 9, Loss: 4.050167560577393\n",
            "Generated text:\n",
            "this movie is one of the best films ever made . it ' s about the film that was not so good in this movie . the story is not so good in it ' s good , but the plot is just\n",
            "\n",
            "Epoch 10, Loss: 3.9843904972076416\n",
            "Generated text:\n",
            "this movie is one of the best films ever made . it ' s about the film that was not so good in this movie . the story is not so good in it ' s a very funny film . i have\n",
            "\n",
            "Epoch 11, Loss: 3.9237399101257324\n",
            "Generated text:\n",
            "this movie is about a group of friends , and it is still not as good as well as good as i can say about this movie is a bad , and the story line that is so much better than i have\n",
            "\n",
            "Epoch 12, Loss: 3.8674674034118652\n",
            "Generated text:\n",
            "this movie is about a group of friends and friends are all over their years old , and the only ones i can ' t understand . the acting was good and the acting was just bad . it ' s the only\n",
            "\n",
            "Epoch 13, Loss: 3.8154399394989014\n",
            "Generated text:\n",
            "this movie is not the worst movie i have ever seen . it ' s so - called \" horror \" and the muppet christmas story . it is the first of all time i can say about it , and i think\n",
            "\n",
            "Epoch 14, Loss: 3.767118453979492\n",
            "Generated text:\n",
            "this movie is great ! i think the idea is the movie for a long time . the movie was about the plot , but it is just the way it is very hard to watch . it ' s all i think\n",
            "\n",
            "Epoch 15, Loss: 3.721898317337036\n",
            "Generated text:\n",
            "this movie is not so bad , but i was shocked at the promos and <UNK> <UNK> <UNK> <UNK> <UNK> as the \" of the two <UNK> <UNK> <UNK> <UNK> \" , and \" i love this film \" is one of my\n",
            "\n",
            "Epoch 16, Loss: 3.678537130355835\n",
            "Generated text:\n",
            "this movie is great ! i think the acting was bad , the movie was good , and the story is not very well written , and well - written , but the film was about as the young girl . it '\n",
            "\n",
            "Epoch 17, Loss: 3.636322259902954\n",
            "Generated text:\n",
            "this movie is about a group of people in this movie , it is very bad , but i can ' t think that they have to do it ? ? the plot ? the movie has no sense of humour and acting\n",
            "\n",
            "Epoch 18, Loss: 3.5961153507232666\n",
            "Generated text:\n",
            "this movie is not just the worst movie i ' ve ever seen . i have never seen any of the plot and acting is just too bad , and it is just not worth a watch . it ' s just awful\n",
            "\n",
            "Epoch 19, Loss: 3.5582656860351562\n",
            "Generated text:\n",
            "this movie is about a man who wants to be the most successful television show , and it ' s the worst i ' ve ever seen . it is the acting in the film . it is very poorly done , and\n",
            "\n",
            "Epoch 20, Loss: 3.52276873588562\n",
            "Generated text:\n",
            "this movie is not the worst movie i ' ve ever seen . it ' s not the worst of the genre . it is not even worth watching , it . it is not only the story of the first film ,\n",
            "\n",
            "Epoch 21, Loss: 3.4889092445373535\n",
            "Generated text:\n",
            "this movie is about a man ' s castle , but i can ' t think it is possible to have to say it ' s not funny , but i don ' t think this movie is one of the best shows\n",
            "\n",
            "Epoch 22, Loss: 3.456512928009033\n",
            "Generated text:\n",
            "this movie is not one of the most <UNK> of the time i have seen . it is the most <UNK> <UNK> of the movies ever made . it is so <UNK> that it is the same , the film has the best\n",
            "\n",
            "Epoch 23, Loss: 3.425560474395752\n",
            "Generated text:\n",
            "this movie is not so bad i could say it is the same as good as the \" lonesome dove \" . it ' s the first movie , i have no idea that this is just not . the story is not\n",
            "\n",
            "Epoch 24, Loss: 3.396376132965088\n",
            "Generated text:\n",
            "this movie is not so bad i could say it is good for a while , i was really expecting something from a different view of what a movie , and that is not . it ' s not the case with this\n",
            "\n",
            "Epoch 25, Loss: 3.368428945541382\n",
            "Generated text:\n",
            "this movie is not so bad i could say , i would like to watch it again . it is the first movie that i saw was very much of a film , but it is still an absolute gem . i think\n",
            "\n",
            "Final generated text:\n",
            "this movie is not so bad i could say , i would like to watch it again . it is the first movie that i saw was very much of a film , but it is still an absolute gem . i think\n"
          ]
        }
      ],
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        "  # You can add additional metrics for tracking\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "generated_text = generate_text(\n",
        "    model, 40, start_tokens, index_to_word\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "num_epochs = 25\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in text_ds:\n",
        "        input_batch = jnp.array(batch)\n",
        "        target_batch = jnp.array([tokens[1:] + [word_to_index['<PAD>']] for tokens in batch])\n",
        "        train_step(model, optimizer, metrics, (input_batch, target_batch))\n",
        "\n",
        "    for metric, value in metrics.compute().items():  # compute metrics\n",
        "      metrics_history[f'train_{metric}'].append(value)  # record metrics\n",
        "    metrics.reset()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {metrics_history['train_loss'][-1]}\")\n",
        "    start_prompt = \"this movie is\"\n",
        "    start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "    generated_text = generate_text(\n",
        "        model, 40, start_tokens, index_to_word\n",
        "    )\n",
        "    print(f\"Generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "# Final text generation\n",
        "start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "generated_text = generate_text(\n",
        "    model, 40, start_tokens, index_to_word\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-ExEt1Zl1C"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sentences that look like sensible movie reviews at the end of the training. Of course the reviews are far from perfect because this model is really small and fundamentally lacks strong intelligence like modern LLMs. In our next tutorial, we are going to scale the model up and make it smarter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKD3c1UMu7x"
      },
      "source": [
        "## Save the model\n",
        "\n",
        "We use [Orbax](https://github.com/google/orbax) to save the model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkoFGCgSZ1yz",
        "outputId": "cb85e0b6-4ba3-44d9-d576-8160920d0c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_CHECKPOINT_METADATA  d  manifest.ocdbt  _METADATA  ocdbt.process_0  _sharding\n"
          ]
        }
      ],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/save', state)\n",
        "\n",
        "# Make sure the files are there\n",
        "!ls /content/save/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zrue6HWMwkG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}